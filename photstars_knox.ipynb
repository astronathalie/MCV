{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l3/kynmmffn6mb9tjg_c1n4qtd80000gn/T/ipykernel_91679/57300138.py:4: DeprecationWarning: `photutils.Background2D` is a deprecated alias for `photutils.background.Background2D` and will be removed in the future. Instead, please use `from photutils.background import Background2D` to silence this warning.\n",
      "  from photutils import Background2D, MedianBackground\n",
      "/var/folders/l3/kynmmffn6mb9tjg_c1n4qtd80000gn/T/ipykernel_91679/57300138.py:4: DeprecationWarning: `photutils.MedianBackground` is a deprecated alias for `photutils.background.MedianBackground` and will be removed in the future. Instead, please use `from photutils.background import MedianBackground` to silence this warning.\n",
      "  from photutils import Background2D, MedianBackground\n"
     ]
    }
   ],
   "source": [
    "from astropy.stats import sigma_clipped_stats\n",
    "from astropy.io import fits\n",
    "from photutils.aperture import ApertureStats, CircularAperture, CircularAnnulus, aperture_photometry, EllipticalAperture, EllipticalAnnulus\n",
    "from photutils import Background2D, MedianBackground\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.stats import SigmaClip\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.wcs import WCS\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from astropy.time import Time\n",
    "from photutils.profiles import RadialProfile\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "#### This is long but it is where all the juicy parts are (photometry, error calculation, conversion to magnitude, etc)\n",
    "#### User should not need to change any of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotometryPipeline:\n",
    "    \"\"\"\n",
    "    Encapsulates relative photometry steps: performing photometry,\n",
    "    calculating aperture-scale ratios, and finding magnitudes.\n",
    "    \"\"\"\n",
    "    def __Init__(self,\n",
    "                 file_dir: str,\n",
    "                 star_list_pix: list,\n",
    "                 ref_Image: str = None,\n",
    "                 target_name: str = None,\n",
    "                 aperture_radius: float = 12,\n",
    "                 annulus_Inner: float = 32,\n",
    "                 annulus_outer: float = 40,\n",
    "                 afactor: float = 1.0,\n",
    "                 bfactor: float = 1.0,\n",
    "                 gap: float = 5.0,\n",
    "                 theta: float = 0,\n",
    "                 gain: float = 0.27,\n",
    "                 rdnoise: float = 5.0):\n",
    "        self.file_dir = file_dir\n",
    "        self.star_list_pix = star_list_pix\n",
    "        self.ref_Image = ref_Image\n",
    "        self.target_name = target_name\n",
    "        self.ap_radius = aperture_radius\n",
    "        self.an_Inner = annulus_Inner\n",
    "        self.an_outer = annulus_outer\n",
    "        self.a = afactor\n",
    "        self.b = bfactor\n",
    "        self.gap = gap\n",
    "        self.theta = theta\n",
    "        self.gain = gain\n",
    "        self.rdnoise = rdnoise\n",
    "        '''The above parameters are default values and can be changed if needed\n",
    "        The value are set when you run it.\n",
    "        The gain and rdnoise are set to correct values (MAY 2025 RLMT) but can be changed if needed\n",
    "        GAIN will read from the header if set to None, this is currently set to use 4*egain due to binning\n",
    "        '''\n",
    "\n",
    "    def _load_Images(self, filt):\n",
    "        path = os.path.join(self.file_dir, filt) + '/'\n",
    "        patterns = ['*.fts', '*.fit', '*.fits']\n",
    "        files = []\n",
    "        for p in patterns:\n",
    "            files += glob.glob(os.path.join(path, p))\n",
    "        return sorted(files)\n",
    "\n",
    "    def _read_header(self, fname):\n",
    "        hdr = fits.getheader(fname)\n",
    "        if self.gain is None:\n",
    "            egain = hdr.get('EGAIN', 1)\n",
    "            self.gain = 4 * egain\n",
    "        return hdr\n",
    "    \n",
    "    def perform_phot(self, filt, \n",
    "                     save=True, \n",
    "                     plot=True, \n",
    "                     save_ref_star_coords=False, \n",
    "                     display_apertures=False, \n",
    "                     HJD = False, \n",
    "                     use_hdr_ap = False,\n",
    "                     large_ap = False):\n",
    "        '''\n",
    "        Perform relative photometry on a set of images\n",
    "        Variable FWHM photometry in different defn\n",
    "        By default the code will save to a csv file and plot results, this can be turned off with flags\n",
    "        '''\n",
    "        images = self._load_Images(filt)\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"No images for filter {filt}\")\n",
    "        ref = self.ref_Image or images[0]\n",
    "        hdr = self._read_header(ref)\n",
    "        wcs = WCS(hdr)\n",
    "        # convert pix->world->pix for all\n",
    "        coords = [wcs.all_pix2world(x, y, 0) for x, y in self.star_list_pix]\n",
    "        # add the target star to the list\n",
    "        if self.target_name is None:\n",
    "            try:\n",
    "                target_name = hdr['BLKNAME']\n",
    "            except KeyError:\n",
    "                target_name = hdr['OBJECT']\n",
    "            print('Target Name:', target_name)\n",
    "        else:\n",
    "            target_name = self.target_name\n",
    "            print('Target Name:', target_name)\n",
    "        # Convert the target coordinates to degrees\n",
    "        target = SkyCoord.from_name(target_name)\n",
    "        ra = target.ra.deg\n",
    "        dec = target.dec.deg\n",
    "        print('Target RA:', ra)\n",
    "        print('Target Dec:', dec)\n",
    "        coords = [(ra, dec)] + coords\n",
    "        all_results = []\n",
    "        for im in images:\n",
    "            data = fits.getdata(im)\n",
    "            hdr = fits.getheader(im)\n",
    "            wcs = WCS(hdr)\n",
    "            xy = [wcs.all_world2pix(ra, dec, 0) for ra, dec in coords]\n",
    "            if save_ref_star_coords:\n",
    "                star_list_saving = pd.DataFrame(coords, columns=['RA', 'Dec'])\n",
    "                target_name = target_name.replace(' ', '_')\n",
    "                star_list_saving.to_csv(self.file_dir + '/' + target_name + '_ref_star_coords.csv', index=False)\n",
    "                print('Saving reference star coordinates to', target_name + '_ref_star_coords.csv') \n",
    "            exptime = hdr['EXPTIME']\n",
    "            if HJD:\n",
    "                # Get the Julian Date from the header and add the half exposure time to get the mid-exposure time\n",
    "                exptime = hdr['EXPTIME']\n",
    "                exptimedays = exptime/(24*3600)\n",
    "                addjd = exptimedays/2\n",
    "                jd = hdr['HJD'] + addjd\n",
    "            else:\n",
    "                # Initialize a dictionary to store the results for this file\n",
    "                ut_date = header['DATE-OBS'] \n",
    "                # Create an Astropy Time object\n",
    "                t = Time(ut_date, scale='utc')\n",
    "                # Get the Julian Date\n",
    "                jd = t.jd\n",
    "                jd = jd + exptime/(2*86400)\n",
    "            file_results = {'file': im, 'Julian_Date': jd}\n",
    "            sigclip = SigmaClip(sigma=3., maxiters=10)\n",
    "            skip_file = False\n",
    "            for i, (x, y) in enumerate(xy):\n",
    "                if use_hdr_ap:\n",
    "                    ap_r = hdr['AP_R']\n",
    "                    an_I = hdr['AN_R1']\n",
    "                    an_o = hdr['AN_R2']\n",
    "                else:\n",
    "                    ap_r = self.ap_radius\n",
    "                    an_I = self.an_Inner\n",
    "                    an_o = self.an_outer\n",
    "                if large_ap:\n",
    "                    ap_r = ap_r * 2.5\n",
    "                ap = CircularAperture((x, y), r=ap_r)\n",
    "                an = CircularAnnulus((x, y), r_In=an_I, r_out=an_o)\n",
    "                bkg_stats = ApertureStats(data, an, sigma_clip=sigclip)\n",
    "                ap_stats = ApertureStats(data, ap, local_bkg=bkg_stats.median)\n",
    "\n",
    "                # Recentroid the aperture\n",
    "                x, y = ap_stats.centroid\n",
    "            \n",
    "        \n",
    "                aperture = CircularAperture((x, y), r=ap_r)\n",
    "                annulus_aperture = CircularAnnulus((x, y), r_In=an_I, r_out=an_o)\n",
    "            \n",
    "                # Perform aperture photometry\n",
    "                phot_table = aperture_photometry(data, aperture)\n",
    "                bkgstats = ApertureStats(data, annulus_aperture, sigma_clip=sigclip)\n",
    "            \n",
    "                # Calculate the background in the annulus\n",
    "                bkg_mean = bkgstats.mean\n",
    "                bkg_sum = bkg_mean * aperture.area\n",
    "            \n",
    "                # Subtract the pedestal from the background for the error calculation\n",
    "                # bkg_mean_nopedestal = bkg_mean-1000\n",
    "                # bkg_sum_nopedestal = bkg_mean_nopedestal * aperture.area\n",
    "            \n",
    "                # Subtract the background from the aperture photometry\n",
    "                source_sum = phot_table['aperture_sum'][0]*self.gain - bkg_sum*self.gain\n",
    "\n",
    "                # Check if the source sum is negative and skip if it is\n",
    "                if source_sum < 0:\n",
    "                    print(f\"Skipping {im} due to negative source sum, image should be inspected.\")\n",
    "                    skip_file = True\n",
    "                    break\n",
    "\n",
    "                # Error calculation (Poisson noise + background noise + read noise)\n",
    "                error = np.sqrt((phot_table['aperture_sum'][0])*self.gain + ((aperture.area)/annulus_aperture.area)*bkg_sum*self.gain + aperture.area*self.rdnoise**2)\n",
    "\n",
    "                # Optionally turn into a magnitude (not used here but useful for reference)\n",
    "                source_mag = -2.5 * np.log10(source_sum / exptime)\n",
    "                source_mag_err = 1.0857 * error / source_sum\n",
    "\n",
    "                # Store the results with dynamic column names\n",
    "                file_results[f'star_{i}_x'] = x\n",
    "                file_results[f'star_{i}_y'] = y\n",
    "                file_results[f'star_{i}_flux'] = source_sum\n",
    "                file_results[f'star_{i}_error'] = error\n",
    "                file_results[f'star_{i}_background'] = bkg_sum                \n",
    "\n",
    "            # Optionally, display the image with the apertures and annuli (set flag to True)\n",
    "            # This is useful for checking the positions of the stars are correct\n",
    "            if display_apertures:\n",
    "                plt.imshow(data, vmin=np.percentile(data, 5), vmax=np.percentile(data, 99), cmap='viridis')\n",
    "                for (x, y) in self.star_list_pix:\n",
    "                    aperture = CircularAperture((x, y), r=ap_r)\n",
    "                    annulus_aperture = CircularAnnulus((x, y), r_In=an_I, r_out=an_o)\n",
    "                    aperture.plot(color='blue', lw=1.5)\n",
    "                    annulus_aperture.plot(color='red', lw=1.5)\n",
    "                plt.show()   \n",
    "\n",
    "            # Append the results for this file to the list of all results\n",
    "            if not skip_file:\n",
    "                all_results.append(file_results) \n",
    "            if skip_file:\n",
    "                print(f\"Skipping {im} due to negative source sum!, image should be inspected.\")\n",
    "                continue\n",
    "\n",
    "        # Convert the results to a DataFrame for easy analysis\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "\n",
    "        # Rename the columns to remove the 'star_0_' prefix for the first star\n",
    "        # and replace it with 'target_' for clarity\n",
    "        results_df.rename(\n",
    "        columns=lambda c: c.replace(\"star_0_\", \"target_\")\n",
    "        if c.startswith(\"star_0_\") else c,\n",
    "        inplace=True\n",
    "        )\n",
    "        \n",
    "        # add all the comparison stars together\n",
    "        results_df['total_flux'] = results_df['star_1_flux'] + results_df['star_2_flux'] + results_df['star_3_flux'] + results_df['star_4_flux'] + results_df['star_5_flux']\n",
    "    \n",
    "        # calculate the relative flux of the target star\n",
    "        results_df['target_rel_flux']=results_df['target_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the relative flux of the comparison stars\n",
    "        for i in range(1,6):\n",
    "            results_df[f'star_{i}_relflux'] = results_df[f'star_{i}_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the error on the total flux\n",
    "        total_flux_err = np.sqrt(results_df['star_1_error']**2 + results_df['star_2_error']**2 + results_df['star_3_error']**2 + results_df['star_4_error']**2 + results_df['star_5_error']**2)\n",
    "\n",
    "        #calculate the error on the relative flux\n",
    "        results_df['target_relerror'] = (results_df['target_flux']/results_df['total_flux'])*(np.sqrt((results_df['target_error']/results_df['target_flux'])**2 + (total_flux_err/results_df['total_flux'])**2))\n",
    "\n",
    "        # Normalize the relative flux\n",
    "        mean_rel_flux, _, _, = sigma_clipped_stats(results_df['target_rel_flux'], sigma=  2.0)\n",
    "        results_df['norm_target_rel_flux'] = results_df['target_rel_flux']/mean_rel_flux\n",
    "        results_df['norm_target_rel_flux_error'] = results_df['target_relerror']/mean_rel_flux\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        if save:\n",
    "            results_df.to_csv(self.file_dir + 'Results_' + filt + '.csv', index=False)\n",
    "\n",
    "        if plot:\n",
    "            #Plot the relative flux\n",
    "            #plt.plot(results_df['Julian_Date'], results_df['target_rel_flux'], 'o')\n",
    "            plt.errorbar(results_df['Julian_Date'], results_df['target_rel_flux'], yerr=results_df['target_relerror'], fmt='o')\n",
    "            plt.xlabel('Julian Date')\n",
    "            plt.ylabel('Relative Flux')\n",
    "            plt.show()\n",
    "        # Return the results DataFrame\n",
    "        return results_df\n",
    "\n",
    "    def perform_e_ap_phot(self, filt,\n",
    "        save = True,\n",
    "        plot = True,\n",
    "        display_apertures = False, # display the apertures and annuli on the reference image\n",
    "        save_ref_star_coords = False, # save the star list in ra/dec coords for reference image\n",
    "        ):\n",
    "        '''\n",
    "        Function to perform elliptical aperture photometry on the images (for windy or bouncy images)\n",
    "        '''\n",
    "        # load images\n",
    "        images = self._load_Images(filt)\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"No images for filter {filt}\")\n",
    "        # read header and world coords of reference\n",
    "        ref = self.ref_Image or images[0]\n",
    "        hdr = self._read_header(ref)\n",
    "        wcs = WCS(hdr)\n",
    "        coords = [wcs.all_pix2world(x, y, 0) for x, y in self.star_list_pix]\n",
    "        # set default apertures from instance values\n",
    "        a = self.a * self.ap_radius\n",
    "        b = self.b * self.ap_radius\n",
    "        a_In = a + self.gap\n",
    "        a_out = a_In + 5\n",
    "        b_In = b + self.gap\n",
    "        b_out = b_In + 5\n",
    "        if self.target_name is None:\n",
    "            try:\n",
    "                target_name = hdr['BLKNAME']\n",
    "            except KeyError:\n",
    "                target_name = hdr['OBJECT']\n",
    "            print('Target Name:', target_name)\n",
    "        else:\n",
    "            target_name = self.target_name\n",
    "            print('Target Name:', target_name)\n",
    "        # Convert the target coordinates to degrees\n",
    "        target = SkyCoord.from_name(target_name)\n",
    "        ra = target.ra.deg\n",
    "        dec = target.dec.deg\n",
    "        print('Target RA:', ra)\n",
    "        print('Target Dec:', dec)\n",
    "        coords = [(ra, dec)] + coords\n",
    "        all_results = []\n",
    "        # Check if the reference star coordinates are provided\n",
    "        # if ref_star_coords:\n",
    "        #     print('Using reference star coordinates')\n",
    "        #     star_list_ra_dec = pd.read_csv(ref_star_coords).values.tolist()\n",
    "        # Convert the star list from pixel coordinates to RA/Dec coordinates\n",
    "        # Initialize a list to store results for all files\n",
    "        for im in images:\n",
    "            data = fits.getdata(im)\n",
    "            hdr = fits.getheader(im)\n",
    "            wcs = WCS(hdr)\n",
    "            xy = [wcs.all_world2pix(ra, dec, 0) for ra, dec in coords]\n",
    "            if save_ref_star_coords:\n",
    "                star_list_saving = pd.DataFrame(coords, columns=['RA', 'Dec'])\n",
    "                target_name = target_name.replace(' ', '_')\n",
    "                star_list_saving.to_csv(self.file_dir + '/' + target_name + '_ref_star_coords.csv', index=False)\n",
    "                print('Saving reference star coordinates to', target_name + '_ref_star_coords.csv')\n",
    "        \n",
    "            # Pull the exposure time from the header, convert to days \n",
    "            # Get the Julian Date from the header and add the half exposure time to get the mid-exposure time\n",
    "            exptime = hdr['EXPTIME']\n",
    "            exptimedays = exptime/(24*3600)\n",
    "            addjd = exptimedays/2\n",
    "            # Initialize a dictionary to store the results for this file\n",
    "            ut_date = header['DATE-OBS'] \n",
    "            # Create an Astropy Time object\n",
    "            t = Time(ut_date, scale='utc')\n",
    "            # Get the Julian Date\n",
    "            jd = t.jd\n",
    "            jd = jd + exptime/(2*86400)\n",
    "            file_results = {'file': im, 'Julian_Date': (jd+addjd)}\n",
    "\n",
    "            #Make a 2D background model of the image\n",
    "            # bkg_estimator = MedianBackground()\n",
    "            # bkg = Background2D(data, (30, 30), filter_size=(3, 3), sigma_clip=SigmaClip(sigma=3), bkg_estimator=bkg_estimator)\n",
    "            # new_data = data - bkg.background\n",
    "\n",
    "            # Stats for sigma clipping\n",
    "            sigclip = SigmaClip(sigma=3., maxiters=10)\n",
    "\n",
    "            # Perform aperture photometry for each star\n",
    "            for i, (x, y) in enumerate(xy):\n",
    "                ap = EllipticalAperture((x, y), a=a, b=b, theta = self.theta)\n",
    "                an = EllipticalAnnulus((x, y), a_In=a_In, a_out=a_out, b_In=b_In, b_out=b_out, theta = self.theta)\n",
    "                bkg_stats = ApertureStats(data, an, sigma_clip=sigclip)\n",
    "                ap_stats = ApertureStats(data, ap, local_bkg=bkg_stats.median)\n",
    "\n",
    "                # Recentroid the aperture\n",
    "                x, y = ap_stats.centroid #not sure this will work with elliptical apertures\n",
    "            \n",
    "        \n",
    "                aperture = EllipticalAperture((x, y), a= a, b=b, theta=self.theta)\n",
    "                annulus_aperture = EllipticalAnnulus((x, y), a_In=a_In, a_out=a_out, b_In=b_In, b_out=b_out, theta = self.theta)\n",
    "            \n",
    "                # Perform aperture photometry\n",
    "                phot_table = aperture_photometry(data, aperture)\n",
    "                bkgstats = ApertureStats(data, annulus_aperture, sigma_clip=sigclip)\n",
    "            \n",
    "                # Calculate the background in the annulus\n",
    "                bkg_mean = bkgstats.mean\n",
    "                bkg_sum = bkg_mean * aperture.area\n",
    "            \n",
    "                # Subtract the pedestal from the background for the error calculation\n",
    "                #bkg_mean_nopedestal = bkg_mean-1000\n",
    "                #bkg_sum_nopedestal = bkg_mean_nopedestal * aperture.area\n",
    "\n",
    "            \n",
    "                # Subtract the background from the aperture photometry\n",
    "                source_sum = phot_table['aperture_sum'][0]*self.gain - bkg_sum*self.gain\n",
    "\n",
    "                # Check if the source sum is negative and skip if it is\n",
    "                if source_sum < 0:\n",
    "                    print(f\"Skipping {im} due to negative source sum, image should be inspected.\")\n",
    "                    continue\n",
    "\n",
    "                # Error calculation (Poisson noise + background noise (not currently included) + read noise)\n",
    "                error = np.sqrt((phot_table['aperture_sum'][0])*self.gain + (aperture.area/annulus_aperture.area)*bkg_sum*self.gain+aperture.area*self.rdnoise**2)\n",
    "\n",
    "                # Optionally turn into a magnitude (not used here but useful for reference)\n",
    "                source_mag = -2.5 * np.log10(source_sum / exptime)\n",
    "                source_mag_err = 1.0857 * error / source_sum\n",
    "\n",
    "                # Store the results with dynamic column names\n",
    "                file_results[f'star_{i}_x'] = x\n",
    "                file_results[f'star_{i}_y'] = y\n",
    "                file_results[f'star_{i}_flux'] = source_sum\n",
    "                file_results[f'star_{i}_error'] = error\n",
    "                #file_results[f'star_{i}_background'] = bkg_sum\n",
    "\n",
    "            if display_apertures:\n",
    "                plt.imshow(data, vmin=np.percentile(data, 5), vmax=np.percentile(data, 99), cmap='viridis')\n",
    "                for (x, y) in self.star_list_pix:\n",
    "                    aperture = EllipticalAperture((x, y), a=a, b=b, theta=self.theta)\n",
    "                    annulus_aperture = EllipticalAnnulus((x, y), a_In=a_In, a_out=a_out, b_In=b_In, b_out=b_out, theta = self.theta)\n",
    "                    aperture.plot(color='blue', lw=1.5)\n",
    "                    annulus_aperture.plot(color='red', lw=1.5)\n",
    "                plt.show()\n",
    "            # Append the results for this file to the list of all results\n",
    "            all_results.append(file_results)\n",
    "\n",
    "        # Convert the results to a DataFrame for easy analysis\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "\n",
    "        # Rename the columns to remove the 'star_0_' prefix for the first star\n",
    "        # and replace it with 'target_' for clarity\n",
    "        results_df.rename(\n",
    "        columns=lambda c: c.replace(\"star_0_\", \"target_\")\n",
    "        if c.startswith(\"star_0_\") else c,\n",
    "        inplace=True)\n",
    "\n",
    "        # add all the comparison stars together\n",
    "        results_df['total_flux'] = results_df['star_1_flux'] + results_df['star_2_flux'] + results_df['star_3_flux'] + results_df['star_4_flux'] + results_df['star_5_flux']\n",
    "    \n",
    "        # calculate the relative flux of the target star\n",
    "        results_df['target_rel_flux']=results_df['target_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the relative flux of the comparison stars\n",
    "        for i in range(1,6):\n",
    "            results_df[f'star_{i}_relflux'] = results_df[f'star_{i}_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the error on the total flux\n",
    "        total_flux_err = np.sqrt(results_df['star_1_error']**2 + results_df['star_2_error']**2 + results_df['star_3_error']**2 + results_df['star_4_error']**2 + results_df['star_5_error']**2)\n",
    "\n",
    "        #calculate the error on the relative flux\n",
    "        results_df['target_relerror'] = (results_df['target_flux']/results_df['total_flux'])*(np.sqrt((results_df['target_error']/results_df['target_flux'])**2 + (total_flux_err/results_df['total_flux'])**2))\n",
    "\n",
    "        # Normalize the relative flux\n",
    "        mean_rel_flux, _, _, = sigma_clipped_stats(results_df['target_rel_flux'], sigma=  2.0)\n",
    "        results_df['norm_target_rel_flux'] = results_df['target_rel_flux']/mean_rel_flux\n",
    "        results_df['norm_target_rel_flux_error'] = results_df['target_relerror']/mean_rel_flux\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        if save:\n",
    "            results_df.to_csv(self.file_dir + 'Results_' + filt + '.csv', index=False)\n",
    "\n",
    "        if plot:\n",
    "            #Plot the relative flux\n",
    "            #plt.plot(results_df['Julian_Date'], results_df['target_rel_flux'], 'o')\n",
    "            plt.errorbar(results_df['Julian_Date'], results_df['target_rel_flux'], yerr=results_df['target_relerror'], fmt='o')\n",
    "            plt.xlabel('Julian Date')\n",
    "            plt.ylabel('Relative Flux')\n",
    "            plt.show()\n",
    "\n",
    "        # Return the results DataFrame\n",
    "        return results_df\n",
    "\n",
    "    def perform_e_phot(self, filt,\n",
    "        save = True,\n",
    "        plot = True,\n",
    "        display_apertures = False, # display the apertures and annuli on the reference image\n",
    "        save_ref_star_coords = False, # save the star list in ra/dec coords for reference image\n",
    "        ):\n",
    "        '''\n",
    "        Function to perform elliptical aperture photometry on the images (for windy or bouncy images)\n",
    "        this one does a background subtraction\n",
    "        may not be needed but have used it in the past\n",
    "        '''\n",
    "        # load images\n",
    "        images = self._load_Images(filt)\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"No images for filter {filt}\")\n",
    "        # read header and world coords of reference\n",
    "        ref = self.ref_Image or images[0]\n",
    "        hdr = self._read_header(ref)\n",
    "        wcs = WCS(hdr)\n",
    "        coords = [wcs.all_pix2world(x, y, 0) for x, y in self.star_list_pix]\n",
    "        # set default apertures from instance values\n",
    "        a = self.a * self.ap_radius\n",
    "        b = self.b * self.ap_radius\n",
    "        a_In = a + self.gap\n",
    "        a_out = a_In + 5\n",
    "        b_In = b + self.gap\n",
    "        b_out = b_In + 5\n",
    "        if self.target_name is None:\n",
    "            try:\n",
    "                target_name = hdr['BLKNAME']\n",
    "            except KeyError:\n",
    "                target_name = hdr['OBJECT']\n",
    "            print('Target Name:', target_name)\n",
    "        else:\n",
    "            target_name = self.target_name\n",
    "            print('Target Name:', target_name)\n",
    "        # Convert the target coordinates to degrees\n",
    "        target = SkyCoord.from_name(target_name)\n",
    "        ra = target.ra.deg\n",
    "        dec = target.dec.deg\n",
    "        print('Target RA:', ra)\n",
    "        print('Target Dec:', dec)\n",
    "        coords = [(ra, dec)] + coords\n",
    "        all_results = []\n",
    "        # Check if the reference star coordinates are provided\n",
    "        # if ref_star_coords:\n",
    "        #     print('Using reference star coordinates')\n",
    "        #     star_list_ra_dec = pd.read_csv(ref_star_coords).values.tolist()\n",
    "        # Convert the star list from pixel coordinates to RA/Dec coordinates\n",
    "        # Initialize a list to store results for all files\n",
    "        \n",
    "        for im in images:\n",
    "            data = fits.getdata(im)\n",
    "            hdr = fits.getheader(im)\n",
    "            wcs = WCS(hdr)\n",
    "            xy = [wcs.all_world2pix(ra, dec, 0) for ra, dec in coords]\n",
    "            if save_ref_star_coords:\n",
    "                star_list_saving = pd.DataFrame(coords, columns=['RA', 'Dec'])\n",
    "                target_name = target_name.replace(' ', '_')\n",
    "                star_list_saving.to_csv(self.file_dir + '/' + target_name + '_ref_star_coords.csv', index=False)\n",
    "                print('Saving reference star coordinates to', target_name + '_ref_star_coords.csv')\n",
    "        \n",
    "            # Pull the exposure time from the header, convert to days \n",
    "            # Get the Julian Date from the header and add the half exposure time to get the mid-exposure time\n",
    "            exptime = hdr['EXPTIME']\n",
    "            exptimedays = exptime/(24*3600)\n",
    "            addjd = exptimedays/2\n",
    "            # Initialize a dictionary to store the results for this file\n",
    "            ut_date = header['DATE-OBS'] \n",
    "            # Create an Astropy Time object\n",
    "            t = Time(ut_date, scale='utc')\n",
    "            # Get the Julian Date\n",
    "            jd = t.jd\n",
    "            jd = jd + exptime/(2*86400)\n",
    "            file_results = {'file': im, 'Julian_Date': (jd+addjd)}\n",
    "\n",
    "            #Make a 2D background model of the image\n",
    "            bkg_estimator = MedianBackground()\n",
    "            bkg = Background2D(data, (30, 30), filter_size=(3, 3), sigma_clip=SigmaClip(sigma=3), bkg_estimator=bkg_estimator)\n",
    "            new_data = data - bkg.background\n",
    "\n",
    "            # Stats for sigma clipping\n",
    "            sigclip = SigmaClip(sigma=3., maxiters=10)\n",
    "\n",
    "            # Perform aperture photometry for each star\n",
    "            for i, (x, y) in enumerate(xy):\n",
    "                ap = EllipticalAperture((x, y), a=a, b=b, theta = self.theta)\n",
    "                #an = EllipticalAnnulus((x, y), a_In=a_In, a_out=a_out, b_In=b_In, b_out=b_out, theta = self.theta)\n",
    "                #bkg_stats = ApertureStats(data, an, sigma_clip=sigclip)\n",
    "                ap_stats = ApertureStats(new_data, ap)\n",
    "\n",
    "                # Recentroid the aperture\n",
    "                x, y = ap_stats.centroid #not sure this will work with elliptical apertures\n",
    "            \n",
    "        \n",
    "                aperture = EllipticalAperture((x, y), a= a, b=b, theta=self.theta)\n",
    "                #annulus_aperture = EllipticalAnnulus((x, y), a_In=a_In, a_out=a_out, b_In=b_In, b_out=b_out, theta = self.theta)\n",
    "            \n",
    "                # Perform aperture photometry\n",
    "                phot_table = aperture_photometry(new_data, aperture)\n",
    "                #bkgstats = ApertureStats(data, annulus_aperture, sigma_clip=sigclip)\n",
    "            \n",
    "                # Calculate the background in the annulus\n",
    "                #bkg_mean = bkgstats.mean\n",
    "                #bkg_sum = bkg_mean * aperture.area\n",
    "            \n",
    "                # Subtract the pedestal from the background for the error calculation\n",
    "                #bkg_mean_nopedestal = bkg_mean-1000\n",
    "                #bkg_sum_nopedestal = bkg_mean_nopedestal * aperture.area\n",
    "\n",
    "            \n",
    "                # Subtract the background from the aperture photometry\n",
    "                source_sum = phot_table['aperture_sum'][0]*self.gain\n",
    "\n",
    "                # Check if the source sum is negative and skip if it is\n",
    "                if source_sum < 0:\n",
    "                    print(f\"Skipping {im} due to negative source sum, image should be inspected.\")\n",
    "                    continue\n",
    "\n",
    "                # Error calculation (Poisson noise + background noise (not currently included) + read noise)\n",
    "                error = np.sqrt((phot_table['aperture_sum'][0])*self.gain +aperture.area*self.rdnoise**2)\n",
    "\n",
    "                # Optionally turn into a magnitude (not used here but useful for reference)\n",
    "                source_mag = -2.5 * np.log10(source_sum / exptime)\n",
    "                source_mag_err = 1.0857 * error / source_sum\n",
    "\n",
    "                # Store the results with dynamic column names\n",
    "                file_results[f'star_{i}_x'] = x\n",
    "                file_results[f'star_{i}_y'] = y\n",
    "                file_results[f'star_{i}_flux'] = source_sum\n",
    "                file_results[f'star_{i}_error'] = error\n",
    "                #file_results[f'star_{i}_background'] = bkg_sum\n",
    "\n",
    "            if display_apertures:\n",
    "                plt.imshow(new_data, vmin=np.percentile(data, 5), vmax=np.percentile(data, 99), cmap='viridis')\n",
    "                for (x, y) in self.star_list_pix:\n",
    "                    aperture = EllipticalAperture((x, y), a=a, b=b, theta=self.theta)\n",
    "                    #annulus_aperture = EllipticalAnnulus((x, y), a_In=a_In, a_out=a_out, b_In=b_In, b_out=b_out, theta = self.theta)\n",
    "                    aperture.plot(color='blue', lw=1.5)\n",
    "                    #annulus_aperture.plot(color='red', lw=1.5)\n",
    "                plt.show()\n",
    "            # Append the results for this file to the list of all results\n",
    "            all_results.append(file_results)\n",
    "\n",
    "        # Convert the results to a DataFrame for easy analysis\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "\n",
    "        # Rename the columns to remove the 'star_0_' prefix for the first star\n",
    "        # and replace it with 'target_' for clarity\n",
    "        results_df.rename(\n",
    "        columns=lambda c: c.replace(\"star_0_\", \"target_\")\n",
    "        if c.startswith(\"star_0_\") else c,\n",
    "        inplace=True)\n",
    "\n",
    "        # add all the comparison stars together\n",
    "        results_df['total_flux'] = results_df['star_1_flux'] + results_df['star_2_flux'] + results_df['star_3_flux'] + results_df['star_4_flux'] + results_df['star_5_flux']\n",
    "    \n",
    "        # calculate the relative flux of the target star\n",
    "        results_df['target_rel_flux']=results_df['target_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the relative flux of the comparison stars\n",
    "        for i in range(1,6):\n",
    "            results_df[f'star_{i}_relflux'] = results_df[f'star_{i}_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the error on the total flux\n",
    "        total_flux_err = np.sqrt(results_df['star_1_error']**2 + results_df['star_2_error']**2 + results_df['star_3_error']**2 + results_df['star_4_error']**2 + results_df['star_5_error']**2)\n",
    "\n",
    "        #calculate the error on the relative flux\n",
    "        results_df['target_relerror'] = (results_df['target_flux']/results_df['total_flux'])*(np.sqrt((results_df['target_error']/results_df['target_flux'])**2 + (total_flux_err/results_df['total_flux'])**2))\n",
    "\n",
    "        # Normalize the relative flux\n",
    "        mean_rel_flux, _, _, = sigma_clipped_stats(results_df['target_rel_flux'], sigma=  2.0)\n",
    "        results_df['norm_target_rel_flux'] = results_df['target_rel_flux']/mean_rel_flux\n",
    "        results_df['norm_target_rel_flux_error'] = results_df['target_relerror']/mean_rel_flux\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        if save:\n",
    "            results_df.to_csv(self.file_dir + 'Results_' + filt + '.csv', index=False)\n",
    "\n",
    "        if plot:\n",
    "            #Plot the relative flux\n",
    "            #plt.plot(results_df['Julian_Date'], results_df['target_rel_flux'], 'o')\n",
    "            plt.errorbar(results_df['Julian_Date'], results_df['target_rel_flux'], yerr=results_df['target_relerror'], fmt='o')\n",
    "            plt.xlabel('Julian Date')\n",
    "            plt.ylabel('Relative Flux')\n",
    "            plt.show()\n",
    "\n",
    "        # Return the results DataFrame\n",
    "        return results_df\n",
    "\n",
    "\n",
    "\n",
    "    def find_fwhm(self, image, positions, size=30):\n",
    "        \"\"\"\n",
    "        Computes approximate FWHM of stars by:\n",
    "        1) subtracting a clipped background\n",
    "        2) building a 1-D radial profile in a few bins\n",
    "        3) finding the radius where profile crosses half-max\n",
    "        \"\"\"\n",
    "        data = fits.getdata(image).astype(float)\n",
    "        fwhm_list = []\n",
    "        # make a sensible set of annular radii\n",
    "        # (you only need one bin per pixel)\n",
    "        radii = np.arange(size+1)\n",
    "\n",
    "        for x0,y0 in positions:\n",
    "            # centroid the star\n",
    "            ap_stats = ApertureStats(data, CircularAperture((x0, y0), r=10), sigma_clip=SigmaClip(sigma=3))\n",
    "            x0, y0 = ap_stats.centroid\n",
    "            x0,y0 = int(x0), int(y0)\n",
    "            # stamp boundary check\n",
    "            if x0-size<0 or x0+size>=data.shape[1] or y0-size<0 or y0+size>=data.shape[0]:\n",
    "                continue\n",
    "\n",
    "            stamp = data[y0-size:y0+size+1, x0-size:x0+size+1]\n",
    "            # mask out the core so bg estimate isn't biased\n",
    "            yy, xx = np.mgrid[:stamp.shape[0], :stamp.shape[1]]\n",
    "            core = ((yy-size)**2 + (xx-size)**2) < (size/4)**2\n",
    "            _, med, _ = sigma_clipped_stats(stamp, mask=core, sigma=3, maxiters=5)\n",
    "            stamp -= med\n",
    "\n",
    "            # build a quick radial profile\n",
    "            rp = RadialProfile(stamp, (size, size), radii, mask=None)\n",
    "            profile = rp.profile\n",
    "            radius  = rp.radius\n",
    "\n",
    "            # compute half‐maximum\n",
    "            half = profile.max() / 2.0\n",
    "            # find index of the peak\n",
    "            peak_Idx = np.nanargmax(profile)\n",
    "            # look for the first bin below half *after* the peak\n",
    "            candidates = np.where((profile < half) & (radius > radius[peak_Idx]))[0]\n",
    "            if len(candidates) == 0:\n",
    "                # no valid half‐max crossing on the far side\n",
    "                continue\n",
    "            i = candidates[0]\n",
    "            # now linearly interpolate between bin i-1 and i\n",
    "            p1, p2 = profile[i-1], profile[i]\n",
    "            r1, r2 = radius[i-1],   radius[i]\n",
    "            rhalf = r1 + (half - p1) * (r2-r1) / (p2-p1)\n",
    "            # full width at half max:\n",
    "            fwhm_list.append(2 * rhalf)\n",
    "\n",
    "        return fwhm_list\n",
    "    \n",
    "    def perform_var_fwhm_phot(self, filt,\n",
    "        save_ref_star_coords = False,\n",
    "        display_apertures = False,\n",
    "        save = True,\n",
    "        plot = True, # plot the results\n",
    "        ):\n",
    "        '''\n",
    "        Function to perform variable FWHM photometry on images\n",
    "        for images with focus issues \n",
    "        may also work for windy or bouncy images\n",
    "        '''\n",
    "        images = self._load_Images(filt)\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"No images for filter {filt}\")\n",
    "        ref = self.ref_Image or images[0]\n",
    "        hdr = self._read_header(ref)\n",
    "        wcs = WCS(hdr)\n",
    "        # convert pix->world->pix for all\n",
    "        coords = [wcs.all_pix2world(x, y, 0) for x, y in self.star_list_pix]\n",
    "        # add the target star to the list\n",
    "        if self.target_name is None:\n",
    "            try:\n",
    "                target_name = hdr['BLKNAME']\n",
    "            except KeyError:\n",
    "                target_name = hdr['OBJECT']\n",
    "            print('Target Name:', target_name)\n",
    "        else:\n",
    "            target_name = self.target_name\n",
    "            print('Target Name:', target_name)\n",
    "        # Convert the target coordinates to degrees\n",
    "        target = SkyCoord.from_name(target_name)\n",
    "        ra = target.ra.deg\n",
    "        dec = target.dec.deg\n",
    "        print('Target RA:', ra)\n",
    "        print('Target Dec:', dec)\n",
    "        coords = [(ra, dec)] + coords\n",
    "        all_results = []\n",
    "\n",
    "        # Loop over each image file\n",
    "        for im in images:\n",
    "            data = fits.getdata(im)\n",
    "            hdr = fits.getheader(im)\n",
    "            wcs = WCS(hdr)\n",
    "            xy = [wcs.all_world2pix(ra, dec, 0) for ra, dec in coords]\n",
    "            if save_ref_star_coords:\n",
    "                star_list_saving = pd.DataFrame(coords, columns=['RA', 'Dec'])\n",
    "                target_name = target_name.replace(' ', '_')\n",
    "                star_list_saving.to_csv(self.file_dir + '/' + target_name + '_ref_star_coords.csv', index=False)\n",
    "                print('Saving reference star coordinates to', target_name + '_ref_star_coords.csv') \n",
    "            exptime = hdr['EXPTIME']\n",
    "            # Initialize a dictionary to store the results for this file\n",
    "            ut_date = header['DATE-OBS'] \n",
    "            # Create an Astropy Time object\n",
    "            t = Time(ut_date, scale='utc')\n",
    "            # Get the Julian Date\n",
    "            jd = t.jd\n",
    "            jd = jd + exptime/(2*86400)\n",
    "\n",
    "            fwhm = self.find_fwhm(im, positions = xy, size = 30)\n",
    "            medianfwhm = np.median(fwhm)\n",
    "\n",
    "            aperture_radius = max(self.ap_radius, medianfwhm)  # Radius of the aperture\n",
    "            annulus_Inner_radius = max(self.an_Inner, medianfwhm*3)  # Inner radius of the annulus\n",
    "            annulus_outer_radius = max(self.an_outer, annulus_Inner_radius+5)  # Outer radius of the annulus\n",
    "\n",
    "            if aperture_radius > 20:\n",
    "                print(f\"Warning: Aperture radius {aperture_radius} is larger than 20 pixels. Check the image {im}.\")\n",
    "                aperture_radius = 20\n",
    "                annulus_Inner_radius = 25\n",
    "                annulus_outer_radius = 30\n",
    "\n",
    "            # Write aperture and annulus parameters to the header\n",
    "        \n",
    "            hdr['AP_R'] = aperture_radius\n",
    "            hdr['AN_R1'] = annulus_Inner_radius\n",
    "            hdr['AN_R2'] = annulus_outer_radius\n",
    "            fits.update(im, data, hdr)\n",
    "        \n",
    "            file_results = {'file': im, 'Julian_Date': (jd)}\n",
    "        \n",
    "            # Stats for sigma clipping\n",
    "            sigclip = SigmaClip(sigma=3., maxiters=10)\n",
    "            skip_file = False\n",
    "            # Perform aperture photometry for each star\n",
    "            for i, (x, y) in enumerate(xy):\n",
    "                ap = CircularAperture((x, y), r=aperture_radius)\n",
    "                an = CircularAnnulus((x, y), r_In=annulus_Inner_radius, r_out=annulus_outer_radius)\n",
    "                bkg_stats = ApertureStats(data, an, sigma_clip=sigclip)\n",
    "                ap_stats = ApertureStats(data, ap, local_bkg=bkg_stats.median)\n",
    "\n",
    "                # Recentroid the aperture\n",
    "                x, y = ap_stats.centroid\n",
    "            \n",
    "        \n",
    "                aperture = CircularAperture((x, y), r=aperture_radius)\n",
    "                annulus_aperture = CircularAnnulus((x, y), r_In=annulus_Inner_radius, r_out=annulus_outer_radius)\n",
    "            \n",
    "                # Perform aperture photometry\n",
    "                phot_table = aperture_photometry(data, aperture)\n",
    "                bkgstats = ApertureStats(data, annulus_aperture, sigma_clip=sigclip)\n",
    "            \n",
    "                # Calculate the background in the annulus\n",
    "                bkg_mean = bkgstats.mean\n",
    "                bkg_sum = bkg_mean * aperture.area\n",
    "            \n",
    "                # Subtract the pedestal from the background for the error calculation\n",
    "                #bkg_mean_nopedestal = bkg_mean-1000\n",
    "                #bkg_sum_nopedestal = bkg_mean_nopedestal * aperture.area\n",
    "\n",
    "            \n",
    "                # Subtract the background from the aperture photometry\n",
    "                source_sum = phot_table['aperture_sum'][0]*self.gain - bkg_sum*self.gain\n",
    "\n",
    "                # Check if the source sum is negative and skip if it is\n",
    "                if source_sum < 0:\n",
    "                    print(f\"Skipping {im} due to negative source sum, image should be inspected.\")\n",
    "                    skip_file = True\n",
    "                    break\n",
    "\n",
    "                # Error calculation (Poisson noise + background noise + read noise)\n",
    "                error = np.sqrt((phot_table['aperture_sum'][0])*self.gain + ((aperture.area)/annulus_aperture.area)*bkg_sum*self.gain + aperture.area*self.rdnoise**2)\n",
    "\n",
    "                # Optionally turn into a magnitude (not used here but useful for reference)\n",
    "                source_mag = -2.5 * np.log10(source_sum / exptime)\n",
    "                source_mag_err = 1.0857 * error / source_sum\n",
    "\n",
    "                # Store the results with dynamic column names\n",
    "                file_results[f'star_{i}_x'] = x\n",
    "                file_results[f'star_{i}_y'] = y\n",
    "                file_results[f'star_{i}_flux'] = source_sum\n",
    "                file_results[f'star_{i}_error'] = error\n",
    "                file_results[f'star_{i}_background'] = bkg_sum\n",
    "\n",
    "            # Optionally, display the image with the apertures and annuli (set flag to True)\n",
    "            # This is useful for checking the positions of the stars are correct\n",
    "            if display_apertures:\n",
    "                plt.imshow(data, vmin=np.percentile(data, 5), vmax=np.percentile(data, 99), cmap='viridis')\n",
    "                for (x, y) in self.star_list_pix:\n",
    "                    aperture = CircularAperture((x, y), r=aperture_radius)\n",
    "                    annulus_aperture = CircularAnnulus((x, y), r_In=annulus_Inner_radius, r_out=annulus_outer_radius)\n",
    "                    aperture.plot(color='blue', lw=1.5)\n",
    "                    annulus_aperture.plot(color='red', lw=1.5)\n",
    "                plt.show()\n",
    "\n",
    "            # Append the results for this file to the list of all results\n",
    "            if skip_file:\n",
    "                print(f\"Skipping {im} due to negative source sum, image should be inspected.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Append the results for this file to the list of all results\n",
    "                all_results.append(file_results)\n",
    "\n",
    "        # Convert the results to a DataFrame for easy analysis\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "\n",
    "        # Rename the columns to remove the 'star_0_' prefix for the first star\n",
    "        # and replace it with 'target_' for clarity\n",
    "        results_df.rename(\n",
    "        columns=lambda c: c.replace(\"star_0_\", \"target_\")\n",
    "        if c.startswith(\"star_0_\") else c,\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "        # add all the comparison stars together\n",
    "        results_df['total_flux'] = results_df['star_1_flux'] + results_df['star_2_flux'] + results_df['star_3_flux'] + results_df['star_4_flux']+ results_df['star_5_flux']\n",
    "    \n",
    "        # calculate the relative flux of the target star\n",
    "        results_df['target_rel_flux']=results_df['target_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the relative flux of the comparison stars\n",
    "        for i in range(1,6):\n",
    "            results_df[f'star_{i}_relflux'] = results_df[f'star_{i}_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the error on the total flux\n",
    "        total_flux_err = np.sqrt(results_df['star_1_error']**2 + results_df['star_2_error']**2 + results_df['star_3_error']**2 + results_df['star_4_error']**2 + results_df['star_5_error']**2)\n",
    "\n",
    "        #calculate the error on the relative flux\n",
    "        results_df['target_relerror'] = (results_df['target_flux']/results_df['total_flux'])*(np.sqrt((results_df['target_error']/results_df['target_flux'])**2 + (total_flux_err/results_df['total_flux'])**2))\n",
    "\n",
    "        # Normalize the relative flux\n",
    "        mean_rel_flux, _, _, = sigma_clipped_stats(results_df['target_rel_flux'], sigma=  2.0)\n",
    "        results_df['norm_target_rel_flux'] = results_df['target_rel_flux']/mean_rel_flux\n",
    "        results_df['norm_target_rel_flux_error'] = results_df['target_relerror']/mean_rel_flux\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        if save:\n",
    "            results_df.to_csv(self.file_dir + 'Results_' + filt + '.csv', index=False)\n",
    "\n",
    "        if plot:\n",
    "            #Plot the relative flux\n",
    "            #plt.plot(results_df['Julian_Date'], results_df['target_rel_flux'], 'o')\n",
    "            plt.errorbar(results_df['Julian_Date'], results_df['target_rel_flux'], yerr=results_df['target_relerror'], fmt='o')\n",
    "            plt.xlabel('Julian Date')\n",
    "            plt.ylabel('Relative Flux')\n",
    "            plt.show()\n",
    "\n",
    "        # Return the results DataFrame\n",
    "        return results_df\n",
    "    \n",
    "    def calc_ap_ratio(self, filt, mean=True, median=False):\n",
    "        # Try to see if AP_R, AN_R1, and AN_R2 are in the header of self\n",
    "        df_small = self.perform_phot(filt, save=False, plot=False, use_hdr_ap=True, large_ap=False)\n",
    "        df_large = self.perform_phot(filt, save=False, plot=False, use_hdr_ap=True, large_ap=True)\n",
    "\n",
    "        df = df_small.merge(df_large, on='file', suffixes=('_small','_large'))\n",
    "        ratio_cols = [f'star_{i}_flux_ratio' for i in range(1,6)]\n",
    "        for i in range(1,6):\n",
    "            df[f'star_{i}_flux_ratio'] = (\n",
    "                df[f'star_{i}_flux_large'] / df[f'star_{i}_flux_small']\n",
    "            )\n",
    "            df[f'star_{i}_flux_ratio_err'] = np.sqrt(\n",
    "                (df[f'star_{i}_error_large'] / df[f'star_{i}_flux_large'])**2 +\n",
    "                (df[f'star_{i}_error_small'] / df[f'star_{i}_flux_small'])**2\n",
    "            )\n",
    "\n",
    "        if mean:\n",
    "            df['mean_ratio'] = df[ratio_cols].mean(axis=1)\n",
    "            df['mean_error'] = df[ratio_cols].std(axis=1)\n",
    "            for fn, scale in zip(df['file'], df['mean_ratio']):\n",
    "                fits.setval(fn, ext=0, keyword='SCALE', value=scale,\n",
    "                            comment='Photometric scale factor')\n",
    "            for fn, err in zip(df['file'], df['mean_error']):\n",
    "                fits.setval(fn, ext=0, keyword='SC_ERR', value=err,\n",
    "                            comment='Scale error (std of ratios)')\n",
    "\n",
    "        if median:\n",
    "            df['median_ratio'] = df[ratio_cols].median(axis=1)\n",
    "            df['median_error'] = df[ratio_cols].std(axis=1)\n",
    "            for fn, scale in zip(df['file'], df['median_ratio']):\n",
    "                fits.setval(fn, ext=0, keyword='SCALE', value=scale,\n",
    "                            comment='Photometric scale factor')\n",
    "            for fn, err in zip(df['file'], df['median_error']):\n",
    "                fits.setval(fn, ext=0, keyword='SC_ERR', value=err,\n",
    "                            comment='Scale error (std of ratios)')\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def find_mags(self, filt, save = True, save_ref_star_coords = False, display_apertures = False, HJD = False, use_hdr_ap = True):\n",
    "        '''\n",
    "        Find the apparent magnitudes of the stars in the image\n",
    "        '''\n",
    "        images = self._load_Images(filt)\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"No images for filter {filt}\")\n",
    "        ref = self.ref_Image or images[0]\n",
    "        hdr = self._read_header(ref)\n",
    "        wcs = WCS(hdr)\n",
    "        # convert pix->world->pix for all\n",
    "        coords = [wcs.all_pix2world(x, y, 0) for x, y in self.star_list_pix]\n",
    "        # add the target star to the list\n",
    "        if self.target_name is None:\n",
    "            try:\n",
    "                target_name = hdr['BLKNAME']\n",
    "            except KeyError:\n",
    "                target_name = hdr['OBJECT']\n",
    "            print('Target Name:', target_name)\n",
    "        else:\n",
    "            target_name = self.target_name\n",
    "            print('Target Name:', target_name)\n",
    "        # Convert the target coordinates to degrees\n",
    "        target = SkyCoord.from_name(target_name)\n",
    "        ra = target.ra.deg\n",
    "        dec = target.dec.deg\n",
    "        print('Target RA:', ra)\n",
    "        print('Target Dec:', dec)\n",
    "        coords = [(ra, dec)] + coords\n",
    "        all_results = []\n",
    "        for im in images:\n",
    "            data = fits.getdata(im)\n",
    "            hdr = fits.getheader(im)\n",
    "            wcs = WCS(hdr)\n",
    "            xy = [wcs.all_world2pix(ra, dec, 0) for ra, dec in coords]\n",
    "            if save_ref_star_coords:\n",
    "                star_list_saving = pd.DataFrame(coords, columns=['RA', 'Dec'])\n",
    "                target_name = target_name.replace(' ', '_')\n",
    "                star_list_saving.to_csv(self.file_dir + '/' + target_name + '_ref_star_coords.csv', index=False)\n",
    "                print('Saving reference star coordinates to', + target_name + '_ref_star_coords.csv') \n",
    "            if HJD:\n",
    "                exptime = hdr['EXPTIME']\n",
    "                jd = hdr['HJD'] + exptime/(2*86400)\n",
    "            else:\n",
    "                exptime = hdr['EXPTIME']\n",
    "                # Initialize a dictionary to store the results for this file\n",
    "                ut_date = header['DATE-OBS'] \n",
    "                # Create an Astropy Time object\n",
    "                t = Time(ut_date, scale='utc')\n",
    "                # Get the Julian Date\n",
    "                jd = t.jd\n",
    "                jd = jd + exptime/(2*86400)\n",
    "            file_results = {'file': im, 'Julian_Date': jd}\n",
    "            sigclip = SigmaClip(sigma=3., maxiters=10)\n",
    "            skip_file = False\n",
    "            for i, (x, y) in enumerate(xy):\n",
    "                if use_hdr_ap:\n",
    "                    ap_r = hdr['AP_R']\n",
    "                    an_I = hdr['AN_R1']\n",
    "                    an_o = hdr['AN_R2']\n",
    "                else:\n",
    "                    ap_r = self.ap_radius\n",
    "                    an_I = self.an_Inner\n",
    "                    an_o = self.an_outer\n",
    "                ap = CircularAperture((x, y), r=ap_r)\n",
    "                an = CircularAnnulus((x, y), r_In=an_I, r_out=an_o)\n",
    "                bkg_stats = ApertureStats(data, an, sigma_clip=sigclip)\n",
    "                ap_stats = ApertureStats(data, ap, local_bkg=bkg_stats.median)\n",
    "\n",
    "                # Recentroid the aperture\n",
    "                x, y = ap_stats.centroid\n",
    "            \n",
    "        \n",
    "                aperture = CircularAperture((x, y), r=ap_r)\n",
    "                annulus_aperture = CircularAnnulus((x, y), r_In=an_I, r_out=an_o)\n",
    "            \n",
    "                # Perform aperture photometry\n",
    "                phot_table = aperture_photometry(data, aperture)\n",
    "                bkgstats = ApertureStats(data, annulus_aperture, sigma_clip=sigclip)\n",
    "            \n",
    "                # Calculate the background in the annulus\n",
    "                bkg_mean = bkgstats.mean\n",
    "                bkg_sum = bkg_mean * aperture.area\n",
    "            \n",
    "                # Subtract the pedestal from the background for the error calculation\n",
    "                #bkg_mean_nopedestal = bkg_mean-1000\n",
    "                #bkg_sum_nopedestal = bkg_mean_nopedestal * aperture.area\n",
    "            \n",
    "                # Subtract the background from the aperture photometry\n",
    "                source_sum = phot_table['aperture_sum'][0]*self.gain - bkg_sum*self.gain\n",
    "\n",
    "                # Check if the source sum is negative and skip if it is\n",
    "                if source_sum < 0:\n",
    "                    print(f\"Skipping {im} due to negative source sum, image should be inspected.\")\n",
    "                    skip_file = True\n",
    "                    break\n",
    "\n",
    "                # Error calculation (Poisson noise + background noise + read noise)\n",
    "                error = np.sqrt((phot_table['aperture_sum'][0])*self.gain + ((aperture.area)/annulus_aperture.area)*bkg_sum*self.gain + aperture.area*self.rdnoise**2)\n",
    "\n",
    "                # Optionally turn into a magnitude \n",
    "                hdr = fits.getheader(im)\n",
    "                try:\n",
    "                    scale = hdr['SCALE']\n",
    "                except KeyError:\n",
    "                #    If the scale is not found in the header, set it to 1\n",
    "                    scale = 1.0\n",
    "                    print('Scale not found in header, using default value of 1.0')\n",
    "                try:\n",
    "                    zpmag = hdr['ZPMAG']\n",
    "                except KeyError:\n",
    "                    # If the zero point magnitude is not found in the header, skip this file\n",
    "                    print('Skipping', {im}, ': Zero point magnitude not found in header')\n",
    "                    skip_file = True\n",
    "                    break\n",
    "                try:\n",
    "                    zpmag_err = hdr['Z_ERR']\n",
    "                except KeyError:\n",
    "                    # If the zero point magnitude error is not found in the header, set it to 0\n",
    "                    print('Skipping', {im}, ': Zero point magnitude not found in header')\n",
    "                    skip_file = True\n",
    "                    break \n",
    "                source_mag = -2.5 * np.log10(scale*source_sum / exptime)\n",
    "                source_mag_cor = source_mag + zpmag\n",
    "                source_mag_err = 1.0857 * error / source_sum\n",
    "                source_mag_cor_err = np.sqrt(source_mag_err**2 + zpmag_err**2)\n",
    "\n",
    "                # Store the results with dynamic column names\n",
    "                file_results[f'star_{i}_x'] = x\n",
    "                file_results[f'star_{i}_y'] = y\n",
    "                file_results[f'star_{i}_flux'] = source_sum\n",
    "                file_results[f'star_{i}_error'] = error\n",
    "                file_results[f'star_{i}_background'] = bkg_sum\n",
    "                file_results[f'star_{i}_mag'] = source_mag_cor\n",
    "                file_results[f'star_{i}_mag_error'] = source_mag_cor_err  \n",
    "            if skip_file:\n",
    "                continue\n",
    "            else:\n",
    "                # Append the results for this file to the list of all results\n",
    "                all_results.append(file_results)                             \n",
    "\n",
    "            # Optionally, display the image with the apertures and annuli (set flag to True)\n",
    "            # This is useful for checking the positions of the stars are correct\n",
    "            if display_apertures:\n",
    "                plt.imshow(data, vmin=np.percentile(data, 5), vmax=np.percentile(data, 99), cmap='viridis')\n",
    "                for (x, y) in self.star_list_pix:\n",
    "                    aperture = CircularAperture((x, y), r=ap_r)\n",
    "                    annulus_aperture = CircularAnnulus((x, y), r_In=an_I, r_out=an_o)\n",
    "                    aperture.plot(color='blue', lw=1.5)\n",
    "                    annulus_aperture.plot(color='red', lw=1.5)\n",
    "                plt.show()   \n",
    "\n",
    "            # Append the results for this file to the list of all results\n",
    "            all_results.append(file_results) \n",
    "\n",
    "        # Convert the results to a DataFrame for easy analysis\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        # Rename the columns to remove the 'star_0_' prefix for the first star\n",
    "\n",
    "        # and replace it with 'target_' for clarity\n",
    "        results_df.rename(\n",
    "        columns=lambda c: c.replace(\"star_0_\", \"target_\")\n",
    "        if c.startswith(\"star_0_\") else c,\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "        # add all the comparison stars together\n",
    "        results_df['total_flux'] = results_df['star_1_flux'] + results_df['star_2_flux'] + results_df['star_3_flux'] + results_df['star_4_flux'] + results_df['star_5_flux']\n",
    "    \n",
    "        # calculate the relative flux of the target star\n",
    "        results_df['target_rel_flux']=results_df['target_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the relative flux of the comparison stars\n",
    "        for i in range(1,6):\n",
    "            results_df[f'star_{i}_relflux'] = results_df[f'star_{i}_flux']/results_df['total_flux']\n",
    "\n",
    "        # calculate the error on the total flux\n",
    "        total_flux_err = np.sqrt(results_df['star_1_error']**2 + results_df['star_2_error']**2 + results_df['star_3_error']**2 + results_df['star_4_error']**2 + results_df['star_5_error']**2)\n",
    "\n",
    "        #calculate the error on the relative flux\n",
    "        results_df['target_relerror'] = (results_df['target_flux']/results_df['total_flux'])*(np.sqrt((results_df['target_error']/results_df['target_flux'])**2 + (total_flux_err/results_df['total_flux'])**2))\n",
    "\n",
    "        # Normalize the relative flux\n",
    "        mean_rel_flux, _, _, = sigma_clipped_stats(results_df['target_rel_flux'], sigma=  2.0)\n",
    "        results_df['norm_target_rel_flux'] = results_df['target_rel_flux']/mean_rel_flux\n",
    "        results_df['norm_target_rel_flux_error'] = results_df['target_relerror']/mean_rel_flux\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        if save:\n",
    "            results_df.to_csv(self.file_dir + 'Results_Cal_' + filt + '.csv', index=False)\n",
    "\n",
    "class LightCurvePlotter:\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_relative(file_dir, filt, comp_stars=False, title=None):\n",
    "    # Change to the directory containing the images\n",
    "        os.chdir(file_dir)\n",
    "        print('Current working directory:', os.getcwd())\n",
    "        # Check if the directory exists\n",
    "        if not os.path.isdir(file_dir):\n",
    "            print('Directory does not exist. Exiting.')\n",
    "            return\n",
    "    \n",
    "        # If filters are not provided, set it to ['g', 'r', 'i']\n",
    "        if filt is None:\n",
    "            print('No filters provided. Using default filters: B, V, R, I')\n",
    "            filt = ['B', 'V', 'R', 'I']\n",
    "        else:\n",
    "            print('Using filters:', filt)\n",
    "\n",
    "        # Check if B is in the filters list\n",
    "        try:\n",
    "            lc_B = pd.read_csv(file_dir + 'Results_B.csv')\n",
    "            lc_B = lc_B.sort_values(by=['Julian_Date'])\n",
    "            plt.errorbar(lc_B['Julian_Date'], lc_B['norm_target_rel_flux'], yerr=lc_B['norm_target_rel_flux_error'], fmt='o', label='B', color='blue')\n",
    "            plt.plot(lc_B['Julian_Date'], lc_B['norm_target_rel_flux'], color='blue')\n",
    "        except FileNotFoundError:\n",
    "            print('No B filter data found. Skipping B filter.')\n",
    "        # Check if V is in the filters list\n",
    "        try:\n",
    "            lc_V = pd.read_csv(file_dir + 'Results_V.csv')\n",
    "            lc_V = lc_V.sort_values(by=['Julian_Date'])\n",
    "            plt.errorbar(lc_V['Julian_Date'], lc_V['norm_target_rel_flux'], yerr=lc_V['norm_target_rel_flux_error'], fmt='o', label='V', color='green')\n",
    "            plt.plot(lc_V['Julian_Date'], lc_V['norm_target_rel_flux'], color='green')\n",
    "        except FileNotFoundError:\n",
    "            print('No V filter data found. Skipping V filter.')\n",
    "        try:\n",
    "            lc_R = pd.read_csv(file_dir + 'results_R.csv')\n",
    "            lc_R = lc_R.sort_values(by=['Julian_Date'])\n",
    "            plt.errorbar(lc_R['Julian_Date'], lc_R['norm_target_rel_flux'], yerr=lc_R['norm_target_rel_flux_error'], fmt='o', label='R', color='red')\n",
    "            plt.plot(lc_R['Julian_Date'], lc_R['norm_target_rel_flux'], color='red')\n",
    "        except FileNotFoundError:\n",
    "            print('No R filter data found. Skipping R filter.')\n",
    "        try:\n",
    "            lc_I = pd.read_csv(file_dir + 'Results_I.csv')\n",
    "            lc_I = lc_I.sort_values(by=['Julian_Date'])\n",
    "            plt.errorbar(lc_I['Julian_Date'], lc_I['norm_target_rel_flux'], yerr=lc_I['norm_target_rel_flux_error'], fmt='o', label='I', color='purple')\n",
    "            plt.plot(lc_I['Julian_Date'], lc_I['norm_target_rel_flux'], color='purple')\n",
    "        except FileNotFoundError:\n",
    "            print('No I filter data found. Skipping i filter.')\n",
    "\n",
    "        # If comp_stars is True, plot the comparison stars\n",
    "        if comp_stars:\n",
    "            try:\n",
    "                lc_B = pd.read_csv(file_dir + 'Results_B.csv')\n",
    "                lc_B = lc_B.sort_values(by=['Julian_Date'])\n",
    "                plt.scatter(lc_B['Julian_Date'], lc_B['star_1_relflux']/np.mean(lc_B['star_1_relflux']), alpha=0.2, color='C0')\n",
    "                plt.scatter(lc_B['Julian_Date'], lc_B['star_2_relflux']/np.mean(lc_B['star_2_relflux']), alpha=0.2, color='C1')\n",
    "                plt.scatter(lc_B['Julian_Date'], lc_B['star_3_relflux']/np.mean(lc_B['star_3_relflux']), alpha=0.2, color='C2')\n",
    "                plt.scatter(lc_B['Julian_Date'], lc_B['star_4_relflux']/np.mean(lc_B['star_4_relflux']), alpha=0.2, color='C3')\n",
    "                plt.scatter(lc_B['Julian_Date'], lc_B['star_5_relflux']/np.mean(lc_B['star_5_relflux']), alpha=0.2, color='C4')\n",
    "            except FileNotFoundError:\n",
    "                print('No B filter data found. Skipping B filter.')\n",
    "            try:\n",
    "                lc_V = pd.read_csv(file_dir + 'Results_V.csv')\n",
    "                lc_V = lc_V.sort_values(by=['Julian_Date'])\n",
    "                plt.scatter(lc_V['Julian_Date'], lc_V['star_1_relflux']/np.mean(lc_V['star_1_relflux']), alpha=0.2, color='C0')\n",
    "                plt.scatter(lc_V['Julian_Date'], lc_V['star_2_relflux']/np.mean(lc_V['star_2_relflux']), alpha=0.2, color='C1')\n",
    "                plt.scatter(lc_V['Julian_Date'], lc_V['star_3_relflux']/np.mean(lc_V['star_3_relflux']), alpha=0.2, color='C2')\n",
    "                plt.scatter(lc_V['Julian_Date'], lc_V['star_4_relflux']/np.mean(lc_V['star_4_relflux']), alpha=0.2, color='C3')\n",
    "                plt.scatter(lc_V['Julian_Date'], lc_V['star_5_relflux']/np.mean(lc_V['star_5_relflux']), alpha=0.2, color='C4')\n",
    "            except FileNotFoundError:\n",
    "                print('No V filter data found. Skipping V filter.')\n",
    "            try:\n",
    "                lc_R = pd.read_csv(file_dir + 'results_R.csv')\n",
    "                lc_R = lc_R.sort_values(by=['Julian_Date'])\n",
    "                plt.scatter(lc_R['Julian_Date'], lc_R['star_1_relflux']/np.mean(lc_R['star_1_relflux']), alpha=0.2, color='C0')\n",
    "                plt.scatter(lc_R['Julian_Date'], lc_R['star_2_relflux']/np.mean(lc_R['star_2_relflux']), alpha=0.2, color='C1')\n",
    "                plt.scatter(lc_R['Julian_Date'], lc_R['star_3_relflux']/np.mean(lc_R['star_3_relflux']), alpha=0.2, color='C2')\n",
    "                plt.scatter(lc_R['Julian_Date'], lc_R['star_4_relflux']/np.mean(lc_R['star_4_relflux']), alpha=0.2, color='C3')\n",
    "                plt.scatter(lc_R['Julian_Date'], lc_R['star_5_relflux']/np.mean(lc_R['star_5_relflux']), alpha=0.2, color='C4')\n",
    "            except FileNotFoundError:\n",
    "                print('No R filter data found. Skipping r filter.')\n",
    "            try:\n",
    "                lc_I = pd.read_csv(file_dir + 'Results_I.csv')\n",
    "                lc_I = lc_I.sort_values(by=['Julian_Date'])\n",
    "                plt.scatter(lc_I['Julian_Date'], lc_I['star_1_relflux']/np.mean(lc_I['star_1_relflux']), alpha=0.2, color='C0')\n",
    "                plt.scatter(lc_I['Julian_Date'], lc_I['star_2_relflux']/np.mean(lc_I['star_2_relflux']), alpha=0.2, color='C1')\n",
    "                plt.scatter(lc_I['Julian_Date'], lc_I['star_3_relflux']/np.mean(lc_I['star_3_relflux']), alpha=0.2, color='C2')\n",
    "                plt.scatter(lc_I['Julian_Date'], lc_I['star_4_relflux']/np.mean(lc_I['star_4_relflux']), alpha=0.2, color='C3')\n",
    "                plt.scatter(lc_I['Julian_Date'], lc_I['star_5_relflux']/np.mean(lc_I['star_5_relflux']), alpha=0.2, color='C4')\n",
    "            except FileNotFoundError:\n",
    "                print('No I filter data found. Skipping I filter.')\n",
    " \n",
    "        # Give labels to the axes and a title\n",
    "        if title is None:\n",
    "            plt.title('Light Curve')\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_absolute(file_dir, filt, title=None):\n",
    "                \n",
    "        '''\n",
    "        Plots the light curve using the apparent magnitudes\n",
    "        '''\n",
    "        \n",
    "        if title is None:\n",
    "            title = 'Absolute Light Curve'\n",
    "        # Change to the directory containing the images\n",
    "        os.chdir(file_dir)\n",
    "        print('Current working directory:', os.getcwd())\n",
    "        # Check if the directory exists\n",
    "        if not os.path.isdir(file_dir):\n",
    "            print('Directory does not exist. Exiting.')\n",
    "            return\n",
    "        \n",
    "        # If filters are not provided, set it to ['g', 'r', 'i']\n",
    "        if filt is None:\n",
    "            print('No filters provided. Using default filters: B, V, R, I')\n",
    "            filt = ['B', 'V', 'R', 'I']\n",
    "        else:\n",
    "            print('Using filters:', filt)\n",
    "        \n",
    "        # if 'g' in the filters list\n",
    "        try:\n",
    "            results_B = pd.read_csv(file_dir + '/Results_Cal_B.csv')\n",
    "            new_results_B_df = pd.DataFrame({\n",
    "            'file': results_B['file'],\n",
    "            'Julian_Date': results_B['Julian_Date'],\n",
    "            'target_mag_B': results_B['target_mag'],\n",
    "            'target_mag_err_B': results_B['target_mag_error'],})\n",
    "            results_B = new_results_B_df.sort_values('Julian_Date')\n",
    "            plt.errorbar(results_B['Julian_Date'], results_B['target_mag_B'], yerr=results_B['target_mag_err_B'], fmt='o-', label='B', color=\"blue\", markersize=6)\n",
    "        except FileNotFoundError:\n",
    "            print('No B filter data found. Skipping B filter.')\n",
    "        try:\n",
    "            results_V = pd.read_csv(file_dir + '/Results_Cal_V.csv')\n",
    "            new_results_V_df = pd.DataFrame({\n",
    "            'file': results_V['file'],\n",
    "            'Julian_Date': results_V['Julian_Date'],\n",
    "            'target_mag_V': results_V['target_mag'],\n",
    "            'target_mag_err_V': results_V['target_mag_error'],})\n",
    "            results_V = new_results_V_df.sort_values('Julian_Date')\n",
    "            plt.errorbar(results_V['Julian_Date'], results_V['target_mag_V'], yerr=results_V['target_mag_err_V'], fmt='o-', label='V', color=\"green\", markersize=6)\n",
    "        except FileNotFoundError:\n",
    "            print('No V filter data found. Skipping V filter.')\n",
    "        # if 'r' in the filters list\n",
    "        try:\n",
    "            results_R = pd.read_csv(file_dir + '/Results_Cal_R.csv')\n",
    "            new_results_R_df = pd.DataFrame({\n",
    "            'file': results_R['file'],\n",
    "            'Julian_Date': results_R['Julian_Date'],\n",
    "            'target_mag_r': results_R['target_mag'],\n",
    "            'target_mag_err_r': results_R['target_mag_error'],})\n",
    "            results_R = new_results_R_df.sort_values('Julian_Date')\n",
    "            plt.errorbar(results_R['Julian_Date'], results_R['target_mag_r'], yerr=results_R['target_mag_err_r'], fmt='o-', label='R', color=\"red\", markersize=6)\n",
    "        except FileNotFoundError:\n",
    "            print('No r filter data found. Skipping r filter.')\n",
    "        # if 'i' in the filters list\n",
    "        try:\n",
    "            results_I = pd.read_csv(file_dir + '/Results_Cal_I.csv')\n",
    "            new_results_I_df = pd.DataFrame({\n",
    "            'file': results_I['file'],\n",
    "            'Julian_Date': results_I['Julian_Date'],\n",
    "            'target_mag_I': results_I['target_mag'],\n",
    "            'target_mag_err_I': results_I['target_mag_error'],})\n",
    "            results_I = new_results_I_df.sort_values('Julian_Date')\n",
    "            plt.errorbar(results_I['Julian_Date'], results_I['target_mag_I'], yerr=results_I['target_mag_err_I'], fmt='o-', label='I', color=\"purple\", markersize=6)\n",
    "        except FileNotFoundError:\n",
    "            print('No I filter data found. Skipping I filter.')\n",
    "        plt.xlabel('Julian Date')\n",
    "        plt.ylabel('Apparent Magnitude')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "class ZPmagrunner:\n",
    "    \"\"\"\n",
    "    Encapsulates calling an external ZP magnitude script across filter subfolders.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir: str, script_path: str, flags: str = '--writeAVG'):\n",
    "        self.base_dir = base_dir\n",
    "        self.script = script_path\n",
    "        self.flags = flags\n",
    "\n",
    "    def run(self, filt):\n",
    "\n",
    "        orig = os.getcwd()\n",
    "        target_dir = os.path.join(self.base_dir,filt) + '/'\n",
    "        print('Processing folder:', target_dir)\n",
    "        subprocess.run(\n",
    "            ['python', self.script, '.', self.flags],\n",
    "            cwd=target_dir,\n",
    "            check=True\n",
    "            )\n",
    "        os.chdir(orig)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the photometry\n",
    "\n",
    "All you should need to change is basic parameters then comment out anything you don't want to run\n",
    "Running the Zero Point takes a long time so beware.\n",
    "You also may have to change the flags on the zero point as it is a little finnicky.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ZP magnitude script for filter: g\n",
      "Processing folder: /Users/nathaliehaurberg/Data/MagneticCVs/final/ST_LMi_RLMT_2025-05-01/g/\n",
      "Found files: ['maw_ST_LMi_g_60s_2025-05-01T03-19-36.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-23-02.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-26-28.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-29-54.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-33-21.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-36-47.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-40-13.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-43-40.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-47-06.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-50-32.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-53-59.fts', 'maw_ST_LMi_g_60s_2025-05-01T03-57-25.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-00-51.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-04-17.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-07-44.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-11-10.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-14-36.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-18-03.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-21-29.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-24-55.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-28-22.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-31-48.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-35-14.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-38-40.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-42-07.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-45-33.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-48-59.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-52-26.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-55-52.fts', 'maw_ST_LMi_g_60s_2025-05-01T04-59-18.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-02-45.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-06-11.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-09-37.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-13-03.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-16-30.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-19-56.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-23-22.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-26-49.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-30-15.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-33-41.fts', 'maw_ST_LMi_g_60s_2025-05-01T05-37-08.fts']\n",
      "Calculating ZPMAG for maw_ST_LMi_g_60s_2025-05-01T03-19-36.fts\n",
      "\n",
      "        SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "        FROM PhotoObj p\n",
      "        JOIN dbo.fGetObjFromRectEq(166.2090096026267, 24.887248558031416, 166.81765552608672, 25.233647185041672) r ON p.objID = r.objID\n",
      "        WHERE p.clean = 1 AND p.g > 14 AND p.g < 16\n",
      "        ORDER BY p.g ASC\n",
      "        \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2090096026267, 24.887248558031416, 166.81765552608672, 25.233647185041672) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 17\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2090096026267, 24.887248558031416, 166.81765552608672, 25.233647185041672) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 18\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2090096026267, 24.887248558031416, 166.81765552608672, 25.233647185041672) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 19\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "Average ZMAG: 22.527 ± 0.045\n",
      "Linear Fit ZMAG: 22.315 ± 0.057\n",
      "Calculating ZPMAG for maw_ST_LMi_g_60s_2025-05-01T03-23-02.fts\n",
      "\n",
      "        SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "        FROM PhotoObj p\n",
      "        JOIN dbo.fGetObjFromRectEq(166.2078850352813, 24.888232748281396, 166.81714685083003, 25.234239620540844) r ON p.objID = r.objID\n",
      "        WHERE p.clean = 1 AND p.g > 14 AND p.g < 16\n",
      "        ORDER BY p.g ASC\n",
      "        \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2078850352813, 24.888232748281396, 166.81714685083003, 25.234239620540844) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 17\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2078850352813, 24.888232748281396, 166.81714685083003, 25.234239620540844) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 18\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2078850352813, 24.888232748281396, 166.81714685083003, 25.234239620540844) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 19\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "Not enough sources to calculate ZMAG\n",
      "Error calculating ZPMAG: too many values to unpack (expected 5)\n",
      "Calculating ZPMAG for maw_ST_LMi_g_60s_2025-05-01T03-26-28.fts\n",
      "\n",
      "        SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "        FROM PhotoObj p\n",
      "        JOIN dbo.fGetObjFromRectEq(166.20816742631604, 24.887937125057583, 166.8168254695339, 25.234393429966723) r ON p.objID = r.objID\n",
      "        WHERE p.clean = 1 AND p.g > 14 AND p.g < 16\n",
      "        ORDER BY p.g ASC\n",
      "        \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20816742631604, 24.887937125057583, 166.8168254695339, 25.234393429966723) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 17\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20816742631604, 24.887937125057583, 166.8168254695339, 25.234393429966723) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 18\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20816742631604, 24.887937125057583, 166.8168254695339, 25.234393429966723) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 19\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "Not enough sources to calculate ZMAG\n",
      "Error calculating ZPMAG: too many values to unpack (expected 5)\n",
      "Calculating ZPMAG for maw_ST_LMi_g_60s_2025-05-01T03-29-54.fts\n",
      "\n",
      "        SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "        FROM PhotoObj p\n",
      "        JOIN dbo.fGetObjFromRectEq(166.20710921513373, 24.88918213992288, 166.81604445573203, 25.23507907405242) r ON p.objID = r.objID\n",
      "        WHERE p.clean = 1 AND p.g > 14 AND p.g < 16\n",
      "        ORDER BY p.g ASC\n",
      "        \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20710921513373, 24.88918213992288, 166.81604445573203, 25.23507907405242) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 17\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20710921513373, 24.88918213992288, 166.81604445573203, 25.23507907405242) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 18\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20710921513373, 24.88918213992288, 166.81604445573203, 25.23507907405242) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 19\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "Average ZMAG: 22.511 ± 0.025\n",
      "Linear Fit ZMAG: 22.716 ± 0.03\n",
      "Calculating ZPMAG for maw_ST_LMi_g_60s_2025-05-01T03-33-21.fts\n",
      "\n",
      "        SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "        FROM PhotoObj p\n",
      "        JOIN dbo.fGetObjFromRectEq(166.20601613891492, 24.88960677571695, 166.81549359520181, 25.23543589616043) r ON p.objID = r.objID\n",
      "        WHERE p.clean = 1 AND p.g > 14 AND p.g < 16\n",
      "        ORDER BY p.g ASC\n",
      "        \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20601613891492, 24.88960677571695, 166.81549359520181, 25.23543589616043) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 17\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20601613891492, 24.88960677571695, 166.81549359520181, 25.23543589616043) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 18\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.20601613891492, 24.88960677571695, 166.81549359520181, 25.23543589616043) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 19\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "Not enough sources to calculate ZMAG\n",
      "Error calculating ZPMAG: too many values to unpack (expected 5)\n",
      "Calculating ZPMAG for maw_ST_LMi_g_60s_2025-05-01T03-36-47.fts\n",
      "\n",
      "        SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "        FROM PhotoObj p\n",
      "        JOIN dbo.fGetObjFromRectEq(166.2047167755143, 24.89278109143428, 166.8148196045751, 25.23668086425418) r ON p.objID = r.objID\n",
      "        WHERE p.clean = 1 AND p.g > 14 AND p.g < 16\n",
      "        ORDER BY p.g ASC\n",
      "        \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2047167755143, 24.89278109143428, 166.8148196045751, 25.23668086425418) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 17\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2047167755143, 24.89278109143428, 166.8148196045751, 25.23668086425418) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 18\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "\n",
      "            SELECT top 200 p.objID, p.ra, p.dec, p.g, p.clean\n",
      "            FROM PhotoObj p\n",
      "            JOIN dbo.fGetObjFromRectEq(166.2047167755143, 24.89278109143428, 166.8148196045751, 25.23668086425418) r ON p.objID = r.objID\n",
      "            WHERE p.clean = 1 AND p.g > 13 AND p.g < 19\n",
      "            ORDER BY p.g ASC\n",
      "            \n",
      "Not enough sources to calculate ZMAG\n",
      "Error calculating ZPMAG: too many values to unpack (expected 5)\n",
      "Calculating ZPMAG for maw_ST_LMi_g_60s_2025-05-01T03-40-13.fts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nathaliehaurberg/Data/MagneticCVs/MCV/CalcZPMag.py\", line 578, in <module>\n",
      "    calc_zmag, zmag_err, lin_fit_zmag, fit_err, _ = processor.zmag_calc()\n",
      "                                                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/Data/MagneticCVs/MCV/CalcZPMag.py\", line 367, in zmag_calc\n",
      "    star_positions = self.find_sources()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/Data/MagneticCVs/MCV/CalcZPMag.py\", line 182, in find_sources\n",
      "    mean, median, std = sigma_clipped_stats(self.data, sigma=5.0)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/astropy/stats/sigma_clipping.py\", line 1021, in sigma_clipped_stats\n",
      "    data_clipped = sigclip(\n",
      "                   ^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/astropy/stats/sigma_clipping.py\", line 677, in __call__\n",
      "    return self._sigmaclip_noaxis(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/astropy/stats/sigma_clipping.py\", line 449, in _sigmaclip_noaxis\n",
      "    self._compute_bounds(filtered_data, axis=None)\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/astropy/stats/sigma_clipping.py\", line 318, in _compute_bounds\n",
      "    self._max_value = self._cenfunc_parsed(data, axis=axis)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/lib/nanfunctions.py\", line 1217, in nanmedian\n",
      "    return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/lib/function_base.py\", line 3823, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/lib/nanfunctions.py\", line 1085, in _nanmedian\n",
      "    return _nanmedian1d(part, overwrite_input)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/lib/nanfunctions.py\", line 1072, in _nanmedian1d\n",
      "    return np.median(arr1d_parsed, overwrite_input=overwrite_input)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/lib/function_base.py\", line 3927, in median\n",
      "    return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/lib/function_base.py\", line 3823, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/lib/function_base.py\", line 3960, in _median\n",
      "    part = partition(a, kth, axis=axis)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nathaliehaurberg/anaconda3/envs/PYTHON3113/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 771, in partition\n",
      "    a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning ZP magnitude script for filter:\u001b[39m\u001b[38;5;124m'\u001b[39m, filt)\n\u001b[1;32m     41\u001b[0m     zp_runner \u001b[38;5;241m=\u001b[39m ZPmagrunner(base_dir\u001b[38;5;241m=\u001b[39mBASE, script_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/nathaliehaurberg/Data/MagneticCVs/MCV/CalcZPMag.py\u001b[39m\u001b[38;5;124m'\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--rewriteAVG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mzp_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Loop over the filters and perform the photometry\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# for filt in filters:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#     print('Performing photometry for filter:', filt)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#     LightCurvePlotter.plot_relative(file_dir=BASE, filt=filt, title = 'ST LMi May 1 2025', comp_stars=True)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#     LightCurvePlotter.plot_absolute(file_dir=BASE, title = 'ST LMi May 1 2025', filt=filt)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 1239\u001b[0m, in \u001b[0;36mZPmagrunner.run\u001b[0;34m(self, filt)\u001b[0m\n\u001b[1;32m   1237\u001b[0m target_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir,filt) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing folder:\u001b[39m\u001b[38;5;124m'\u001b[39m, target_dir)\n\u001b[0;32m-> 1239\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(orig)\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON3113/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON3113/lib/python3.11/subprocess.py:1201\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON3113/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON3113/lib/python3.11/subprocess.py:2046\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2045\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2046\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON3113/lib/python3.11/subprocess.py:2004\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2004\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, wait_flags)\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2008\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2009\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the directory containing the image folders and the filt\n",
    "BASE = '/Users/nathaliehaurberg/Data/MagneticCVs/final/ST_LMi_RLMT_2025-05-01/'\n",
    "\n",
    "# set the star list in pixels\n",
    "STARS = [(2900.2411,1477.3404), (2986.2712,1509.9692), (2412.4599,1996.4578), (2802.4384,1234.5056),(2245.5118,1043.4673)]\n",
    "\n",
    "# set the reference image\n",
    "REFERENCE_IMAGE = '/Users/nathaliehaurberg/Data/MagneticCVs/final/ST_LMi_RLMT_2025-02-28/g/mcr_ST_LMi_g_60s_2025-02-28T03-20-13.fts'\n",
    "\n",
    "# Set the target name\n",
    "TARGET_NAME = 'ST_LMi'\n",
    "\n",
    "# Set the gain and read noise\n",
    "RDNOISE = 5.0\n",
    "GAIN = 0.27\n",
    "# Gain will be found interactively approipriately for this camera\n",
    "\n",
    "# Set the aperture and annulus radii\n",
    "AP_R = 12\n",
    "AN_I = 32\n",
    "AN_O = 40\n",
    "\n",
    "# Set the filters\n",
    "# filters = glob.glob(BASE + '/*/')\n",
    "# filters = [os.path.basename(f.rstrip('/\\\\')) for f in filters]\n",
    "filters = ['B', 'V', 'R', 'I']\n",
    "\n",
    "pipe = PhotometryPipeline(\n",
    "    file_dir=BASE,\n",
    "    star_list_pix=STARS,\n",
    "    ref_image=REFERENCE_IMAGE,\n",
    "    target_name=TARGET_NAME,\n",
    "    rdnoise=RDNOISE,\n",
    "    aperture_radius=AP_R,\n",
    "    annulus_inner=AN_I,\n",
    "    annulus_outer=AN_O\n",
    ")\n",
    "\n",
    "#Run the ZP magnitude script for each filter\n",
    "# for filt in filters:\n",
    "#     print('Running ZP magnitude script for filter:', filt)\n",
    "#     zp_runner = ZPmagrunner(base_dir=BASE, script_path='/Users/nathaliehaurberg/Data/MagneticCVs/MCV/CalcZPMag.py', flags='--rewriteAVG')\n",
    "#     zp_runner.run(filt)\n",
    "\n",
    "# Loop over the filters and perform the photometry\n",
    "for filt in filters:\n",
    "    print('Performing photometry for filter:', filt)\n",
    "    pipe.perform_var_fwhm_phot(filt, save_ref_star_coords=False, display_apertures=False, save=True, plot=True)\n",
    "    pipe.calc_ap_ratio(filt, mean=True, median=False)\n",
    "    pipe.find_mags(filt, save=True, save_ref_star_coords=False, display_apertures=False)\n",
    "    LightCurvePlotter.plot_relative(file_dir=BASE, filt=filt, title = 'ST LMi May 1 2025', comp_stars=True)\n",
    "    LightCurvePlotter.plot_absolute(file_dir=BASE, title = 'ST LMi May 1 2025', filt=filt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTHON3113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
